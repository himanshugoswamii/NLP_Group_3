{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 - FINAL\n",
        "\n",
        "def preprocessing(text):\n",
        "    text = text.lower()\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def unigram_and_bigram(corpus_path):\n",
        "    unigram_counts = {}\n",
        "    bigram_counts = {}\n",
        "    with open(corpus_path, \"r\") as f:\n",
        "        for review in f:\n",
        "            tokens = preprocessing(review.strip())\n",
        "            # unigrams\n",
        "            for token in tokens:\n",
        "                unigram_counts[token] = unigram_counts.get(token, 0) + 1\n",
        "            # bigrams\n",
        "            for i in range(len(tokens) - 1):\n",
        "                bigram = (tokens[i], tokens[i + 1])\n",
        "                bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "    return unigram_counts, bigram_counts\n",
        "\n",
        "\n",
        "def computing_probabilities(unigram_counts, bigram_counts):\n",
        "    total_unigrams = sum(unigram_counts.values())\n",
        "\n",
        "    # unigram\n",
        "    unigram_prob = {}\n",
        "    for word, count in unigram_counts.items():\n",
        "        unigram_prob[word] = count / total_unigrams\n",
        "\n",
        "    # bigram\n",
        "    bigram_prob = {}\n",
        "    for (word1, word2), count in bigram_counts.items():\n",
        "        if word1 not in bigram_prob:\n",
        "            bigram_prob[word1] = {}\n",
        "        bigram_prob[word1][word2] = count / unigram_counts[word1]\n",
        "\n",
        "    return unigram_prob, bigram_prob\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_file = \"/content/train.txt\"\n",
        "    unigram_counts, bigram_counts = unigram_and_bigram(train_file)\n",
        "    unigram_prob, bigram_prob = computing_probabilities(unigram_counts, bigram_counts)\n",
        "\n",
        "    # printing top 10 unigrams\n",
        "    print(\"Top 10 unigram probabilities:\")\n",
        "    for word, prob in list(unigram_prob.items())[:10]:\n",
        "        print(f\"P({word}) = {prob:.4f}\")\n",
        "\n",
        "    # printing bigram probabilities\n",
        "    test_word = \"the\"\n",
        "    if test_word in bigram_prob:\n",
        "        print(f\"\\nBigram probabilities for context '{test_word}':\")\n",
        "        for word, prob in list(bigram_prob[test_word].items())[:10]:\n",
        "            print(f\"P({word}|{test_word}) = {prob:.4f}\")\n"
      ],
      "metadata": {
        "id": "ryFITUYqJ22f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cc5a53f-0e6f-4722-c6b1-99b04b1ae89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 unigram probabilities:\n",
            "P(i) = 0.0190\n",
            "P(booked) = 0.0010\n",
            "P(two) = 0.0014\n",
            "P(rooms) = 0.0022\n",
            "P(four) = 0.0002\n",
            "P(months) = 0.0001\n",
            "P(in) = 0.0140\n",
            "P(advance) = 0.0001\n",
            "P(at) = 0.0083\n",
            "P(the) = 0.0590\n",
            "\n",
            "Bigram probabilities for context 'the':\n",
            "P(talbott|the) = 0.0049\n",
            "P(top|the) = 0.0023\n",
            "P(elevators|the) = 0.0021\n",
            "P(front|the) = 0.0183\n",
            "P(16th|the) = 0.0004\n",
            "P(noise|the) = 0.0019\n",
            "P(hotel|the) = 0.0780\n",
            "P(elevator|the) = 0.0038\n",
            "P(room|the) = 0.0557\n",
            "P(bed|the) = 0.0119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 - FINAL\n",
        "\n",
        "def preprocessing(corpus_path, unk_threshold=1):\n",
        "    unigram_counts = {}\n",
        "    tokenized_sentences = []\n",
        "    with open(corpus_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            tokens = line.strip().split()\n",
        "            tokens = [t.lower() for t in tokens]\n",
        "\n",
        "            for token in tokens:\n",
        "                unigram_counts[token] = unigram_counts.get(token, 0) + 1\n",
        "\n",
        "            tokenized_sentences.append(tokens)\n",
        "\n",
        "    # vocabulary with <UNK>\n",
        "    vocab = set()\n",
        "    for word, count in unigram_counts.items():\n",
        "        if count > unk_threshold:\n",
        "            vocab.add(word)\n",
        "    vocab.add(\"<UNK>\")\n",
        "\n",
        "    # rare words to <UNK>\n",
        "    modified_sentences = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        processed = []\n",
        "        for word in sentence:\n",
        "            if word in vocab:\n",
        "                processed.append(word)\n",
        "            else:\n",
        "                processed.append(\"<UNK>\")\n",
        "        modified_sentences.append(processed)\n",
        "\n",
        "    return modified_sentences, vocab\n",
        "\n",
        "\n",
        "\n",
        "def count_ngrams(sentences):\n",
        "    unigram_counts = {}\n",
        "    bigram_counts = {}\n",
        "    for tokens in sentences:\n",
        "        tokens = [\"<s>\"] + tokens + [\"</s>\"]\n",
        "\n",
        "        # unigrams\n",
        "        for token in tokens:\n",
        "            unigram_counts[token] = unigram_counts.get(token, 0) + 1\n",
        "\n",
        "        # bigrams\n",
        "        for i in range(len(tokens) - 1):\n",
        "            bigram = (tokens[i], tokens[i+1])\n",
        "            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "    return unigram_counts, bigram_counts\n",
        "\n",
        "\n",
        "def bigram_laplace(bigram_counts, unigram_counts, vocab_size):\n",
        "    bigram_probs = {}\n",
        "    for (word1, word2), count in bigram_counts.items():\n",
        "        if word1 not in bigram_probs:\n",
        "            bigram_probs[word1] = {}\n",
        "        bigram_probs[word1][word2] = (count + 1) / (unigram_counts[word1] + vocab_size)\n",
        "    return bigram_probs\n",
        "\n",
        "\n",
        "def bigram_addk(bigram_counts, unigram_counts, vocab_size, k=0.5):\n",
        "    bigram_probs = {}\n",
        "    for word1 in unigram_counts:\n",
        "        if word1 not in bigram_probs:\n",
        "            bigram_probs[word1] = {}\n",
        "        for word2 in unigram_counts:\n",
        "            count = bigram_counts.get((word1, word2), 0)\n",
        "            bigram_probs[word1][word2] = (count + k) / (unigram_counts[word1] + k * vocab_size)\n",
        "    return bigram_probs\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_file = \"/content/train.txt\"\n",
        "    sentences, vocab = preprocessing(train_file, unk_threshold=1)\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # n-grams\n",
        "    unigram_counts, bigram_counts = count_ngrams(sentences)\n",
        "\n",
        "    # bigram\n",
        "    bigram_laplace = bigram_laplace(bigram_counts, unigram_counts, vocab_size)\n",
        "    bigram_addk = bigram_addk(bigram_counts, unigram_counts, vocab_size, k=0.5)\n",
        "\n",
        "    # printing top 10 unigram\n",
        "    total_unigrams = sum(unigram_counts.values())\n",
        "    unigram_probs = {}\n",
        "    for word, count in unigram_counts.items():\n",
        "        unigram_probs[word] = count / total_unigrams\n",
        "\n",
        "    print(\"Top 10 unigram probabilities:\")\n",
        "    for word, prob in list(unigram_probs.items())[:10]:\n",
        "        print(f\"P({word}) = {prob:.4f}\")\n",
        "\n",
        "    # printing bigram for word 'the'\n",
        "    word = \"the\"\n",
        "    if word in bigram_laplace:\n",
        "        print(f\"\\nBigram probabilities for context '{word}' (Laplace):\")\n",
        "        for word1, prob in list(bigram_laplace[word].items())[:10]:\n",
        "            print(f\"P({word1}|{word}) = {prob:.4f}\")\n",
        "\n",
        "    if word in bigram_addk:\n",
        "        print(f\"\\nBigram probabilities for context '{word}' (Add-0.5):\")\n",
        "        for word1, prob in list(bigram_addk[word].items())[:10]:\n",
        "            print(f\"P({word1}|{word}) = {prob:.4f}\")\n"
      ],
      "metadata": {
        "id": "JEf9swJkJ1mY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "207fe9b6-2087-402e-c6cb-3c4a939f52f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 unigram probabilities:\n",
            "P(<s>) = 0.0056\n",
            "P(i) = 0.0188\n",
            "P(booked) = 0.0009\n",
            "P(two) = 0.0014\n",
            "P(rooms) = 0.0022\n",
            "P(four) = 0.0002\n",
            "P(months) = 0.0001\n",
            "P(in) = 0.0139\n",
            "P(advance) = 0.0001\n",
            "P(at) = 0.0082\n",
            "\n",
            "Bigram probabilities for context 'the' (Laplace):\n",
            "P(talbott|the) = 0.0032\n",
            "P(top|the) = 0.0015\n",
            "P(elevators|the) = 0.0014\n",
            "P(front|the) = 0.0117\n",
            "P(16th|the) = 0.0004\n",
            "P(noise|the) = 0.0013\n",
            "P(hotel|the) = 0.0492\n",
            "P(elevator|the) = 0.0025\n",
            "P(room|the) = 0.0352\n",
            "P(bed|the) = 0.0076\n",
            "\n",
            "Bigram probabilities for context 'the' (Add-0.5):\n",
            "P(<s>|the) = 0.0001\n",
            "P(i|the) = 0.0002\n",
            "P(booked|the) = 0.0001\n",
            "P(two|the) = 0.0007\n",
            "P(rooms|the) = 0.0131\n",
            "P(four|the) = 0.0012\n",
            "P(months|the) = 0.0001\n",
            "P(in|the) = 0.0001\n",
            "P(advance|the) = 0.0001\n",
            "P(at|the) = 0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 3:\n",
        "import math\n",
        "\n",
        "def preprocess_with_unk(corpus_path, unk_threshold=1):\n",
        "    unigram_counts = {}\n",
        "    tokenized_sentences = []\n",
        "\n",
        "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            tokens = line.strip().lower().split()\n",
        "            tokenized_sentences.append(tokens)\n",
        "            for token in tokens:\n",
        "                unigram_counts[token] = unigram_counts.get(token, 0) + 1\n",
        "\n",
        "    vocab = {word for word, count in unigram_counts.items() if count > unk_threshold}\n",
        "    vocab.add(\"<UNK>\")\n",
        "\n",
        "    processed_sentences = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        processed = [w if w in vocab else \"<UNK>\" for w in sentence]\n",
        "        processed_sentences.append(processed)\n",
        "\n",
        "    return processed_sentences, vocab\n",
        "\n",
        "def count_ngrams(sentences):\n",
        "\n",
        "    unigram_counts = {}\n",
        "    bigram_counts = {}\n",
        "\n",
        "    for tokens in sentences:\n",
        "        tokens = [\"<s>\"] + tokens + [\"</s>\"]\n",
        "\n",
        "        for token in tokens:\n",
        "            unigram_counts[token] = unigram_counts.get(token, 0) + 1\n",
        "\n",
        "        for i in range(len(tokens) - 1):\n",
        "            bigram = (tokens[i], tokens[i + 1])\n",
        "            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "    return unigram_counts, bigram_counts\n",
        "\n",
        "def calculate_perplexity(validation_path, model_bigram_counts, model_unigram_counts, vocab, k=1.0):\n",
        "    vocab_size = len(vocab)\n",
        "    log_prob_sum = 0.0\n",
        "    N = 0\n",
        "\n",
        "    with open(validation_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            raw_tokens = line.strip().lower().split()\n",
        "            processed = [word if word in vocab else \"<UNK>\" for word in raw_tokens]\n",
        "            tokens = [\"<s>\"] + processed + [\"</s>\"]\n",
        "\n",
        "            N += len(tokens) - 1\n",
        "\n",
        "            for i in range(1, len(tokens)):\n",
        "                w1 = tokens[i-1]\n",
        "                w2 = tokens[i]\n",
        "\n",
        "\n",
        "                bigram_count = model_bigram_counts.get((w1, w2), 0)\n",
        "                unigram_count = model_unigram_counts.get(w1, 0)\n",
        "\n",
        "                denominator = unigram_count + k * vocab_size\n",
        "                if denominator == 0:\n",
        "                    prob = 1.0 / vocab_size\n",
        "                else:\n",
        "                    prob = (bigram_count + k) / denominator\n",
        "\n",
        "                log_prob_sum += math.log(prob, 2)\n",
        "\n",
        "    if N == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    avg_log_prob = log_prob_sum / N\n",
        "    perplexity = math.pow(2, -avg_log_prob)\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    train_file = \"/content/train.txt\"\n",
        "    validation_file = \"/content/val.txt\"\n",
        "\n",
        "    print(f\"Training model on: {train_file}\")\n",
        "\n",
        "    train_sentences, vocab = preprocess_with_unk(train_file, unk_threshold=1)\n",
        "\n",
        "    model_unigram_counts, model_bigram_counts = count_ngrams(train_sentences)\n",
        "    print(f\"Vocabulary size: {len(vocab)}\")\n",
        "\n",
        "    print(f\"\\nCalculating perplexity for: {validation_file}\")\n",
        "\n",
        "    perplexity_laplace = calculate_perplexity(\n",
        "        validation_file,\n",
        "        model_bigram_counts,\n",
        "        model_unigram_counts,\n",
        "        vocab,\n",
        "        k=1.0\n",
        "    )\n",
        "    print(f\"Perplexity with Laplace (k=1.0) smoothing: {perplexity_laplace:.4f}\")\n",
        "\n",
        "    perplexity_add_k = calculate_perplexity(\n",
        "        validation_file,\n",
        "        model_bigram_counts,\n",
        "        model_unigram_counts,\n",
        "        vocab,\n",
        "        k=0.5\n",
        "    )\n",
        "    print(f\"Perplexity with Add-k (k=0.5) smoothing: {perplexity_add_k:.4f}\")"
      ],
      "metadata": {
        "id": "Cyfm3fe2J-ht",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a14439e-8db2-427b-8a3d-e65b6aed6747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model on: /content/train.txt\n",
            "Vocabulary size: 3116\n",
            "\n",
            "Calculating perplexity for: /content/val.txt\n",
            "Perplexity with Laplace (k=1.0) smoothing: 428.8675\n",
            "Perplexity with Add-k (k=0.5) smoothing: 316.5824\n"
          ]
        }
      ]
    }
  ]
}